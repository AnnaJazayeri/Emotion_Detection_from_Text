{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c3c80e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba80f549",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import re, itertools\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import preprocessor as p\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import little_mallet_wrapper\n",
    "from nltk.tokenize import TweetTokenizer \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.classify import SklearnClassifier\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "pd.options.display.max_colwidth = 100\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8218207",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "import inflect\n",
    "import contractions\n",
    "from bs4 import BeautifulSoup\n",
    "import re, string, unicodedata\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f898c13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b18d1e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bed1b6da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text       0\n",
      "Emotion    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset with column names\n",
    "column_names = ['Text', 'Emotion']\n",
    "train_data = pd.read_excel(\"Tweet Emotion Dataset.xlsx\", names=column_names)\n",
    "\n",
    "# Check for missing values\n",
    "print(train_data.isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e84274a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@ArcticFantasy I would have almost took offense to this if I actually snapped you</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@IllinoisLoyalty that Rutgers game was an abomination. An affront to God and man. We must never ...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@CozanGaming that's what lisa asked before she started raging at me, 'can I call you?' heh</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sometimes I get mad over something so minuscule I try to ruin somebodies life not like lose your...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sometimes I get mad over something so minuscule I try to ruin somebodies life not like lose your...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>Common app just randomly logged me out as I was writing the last part of my college essay and lo...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>I'd rather laugh with the rarest genius, in beautiful alliance with his own being, where he kept...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>If you #invest in my new #film I will stop asking you to invest in my new film. #concessions #cr...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>Just watched Django Unchained, Other people may frown, but I titter in delight! 2/5</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>@KeithOlbermann depressing how despicable Trump, with no policies, campaigning on bigotry &amp;amp; ...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>346 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                    Text  \\\n",
       "0                      @ArcticFantasy I would have almost took offense to this if I actually snapped you   \n",
       "1    @IllinoisLoyalty that Rutgers game was an abomination. An affront to God and man. We must never ...   \n",
       "2             @CozanGaming that's what lisa asked before she started raging at me, 'can I call you?' heh   \n",
       "3    Sometimes I get mad over something so minuscule I try to ruin somebodies life not like lose your...   \n",
       "4    Sometimes I get mad over something so minuscule I try to ruin somebodies life not like lose your...   \n",
       "..                                                                                                   ...   \n",
       "341  Common app just randomly logged me out as I was writing the last part of my college essay and lo...   \n",
       "342  I'd rather laugh with the rarest genius, in beautiful alliance with his own being, where he kept...   \n",
       "343  If you #invest in my new #film I will stop asking you to invest in my new film. #concessions #cr...   \n",
       "344                  Just watched Django Unchained, Other people may frown, but I titter in delight! 2/5   \n",
       "345  @KeithOlbermann depressing how despicable Trump, with no policies, campaigning on bigotry &amp; ...   \n",
       "\n",
       "     Emotion  \n",
       "0      anger  \n",
       "1      anger  \n",
       "2      anger  \n",
       "3      anger  \n",
       "4      anger  \n",
       "..       ...  \n",
       "341  sadness  \n",
       "342  sadness  \n",
       "343  sadness  \n",
       "344  sadness  \n",
       "345  sadness  \n",
       "\n",
       "[346 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "830d935c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfebc36f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Text, Emotion]\n",
       "Index: []"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[train_data.duplicated() == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0127413",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Emotion\n",
       "fear       110\n",
       "anger       83\n",
       "joy         79\n",
       "sadness     74\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['Emotion'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "855a20fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x=train_data['Emotion'])\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e801d985",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing_platform(df, text_col, remove_stopwords=True):\n",
    "    \n",
    "    ## Define functions for individual steps\n",
    "    # First function is used to denoise text\n",
    "    def denoise_text(text):\n",
    "        # Strip html if any. For ex. removing <html>, <p> tags\n",
    "        soup = BeautifulSoup(text, \"html.parser\")\n",
    "        text = soup.get_text()\n",
    "        # Replace contractions in the text. For ex. didn't -> did not\n",
    "        text = contractions.fix(text)\n",
    "        return text\n",
    "    \n",
    "    ## Next step is text-normalization\n",
    "    \n",
    "    # Text normalization includes many steps.\n",
    "    \n",
    "    # Each function below serves a step.\n",
    "    \n",
    "    \n",
    "    def remove_non_ascii(words):\n",
    "        \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "        new_words = []\n",
    "        for word in words:\n",
    "            new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "            new_words.append(new_word)\n",
    "        return new_words\n",
    "    \n",
    "    \n",
    "    def to_lowercase(words):\n",
    "        \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "        new_words = []\n",
    "        for word in words:\n",
    "            new_word = word.lower()\n",
    "            new_words.append(new_word)\n",
    "        return new_words\n",
    "    \n",
    "    \n",
    "    def remove_punctuation(words):\n",
    "        \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "        new_words = []\n",
    "        for word in words:\n",
    "            new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "            if new_word != '':\n",
    "                new_words.append(new_word)\n",
    "        return new_words\n",
    "    \n",
    "    \n",
    "    def replace_numbers(words):\n",
    "        \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "        p = inflect.engine()\n",
    "        new_words = []\n",
    "        for word in words:\n",
    "            if word.isdigit():\n",
    "                new_word = p.number_to_words(word)\n",
    "                new_words.append(new_word)\n",
    "            else:\n",
    "                new_words.append(word)\n",
    "        return new_words\n",
    "    \n",
    "    \n",
    "    def remove_stopwords(words):\n",
    "        \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "        new_words = []\n",
    "        for word in words:\n",
    "            if word not in stopwords.words('english'):\n",
    "                new_words.append(word)\n",
    "        return new_words\n",
    "    \n",
    "    \n",
    "    def stem_words(words):\n",
    "        \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "        stemmer = LancasterStemmer()\n",
    "        stems = []\n",
    "        for word in words:\n",
    "            stem = stemmer.stem(word)\n",
    "            stems.append(stem)\n",
    "        return stems\n",
    "    \n",
    "    \n",
    "    def lemmatize_verbs(words):\n",
    "        \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        lemmas = []\n",
    "        for word in words:\n",
    "            lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "            lemmas.append(lemma)\n",
    "        return lemmas\n",
    "    \n",
    "    \n",
    "    ### A wrap-up function for normalization\n",
    "    def normalize_text(words, remove_stopwords):\n",
    "        words = remove_non_ascii(words)\n",
    "        words = to_lowercase(words)\n",
    "        words = remove_punctuation(words)\n",
    "        words = replace_numbers(words)\n",
    "        if remove_stopwords:\n",
    "            words = remove_stopwords(words)\n",
    "        #words = stem_words(words)\n",
    "        words = lemmatize_verbs(words)\n",
    "        return words\n",
    "    \n",
    "    # All above functions work on word tokens we need a tokenizer\n",
    "    \n",
    "    # Tokenize tweet into words\n",
    "    def tokenize(text):\n",
    "        return nltk.word_tokenize(text)\n",
    "    \n",
    "    \n",
    "    # A overall wrap-up function\n",
    "    def text_prepare(text):\n",
    "        text = denoise_text(text)\n",
    "        text = ' '.join([x for x in normalize_text(tokenize(text), remove_stopwords)])\n",
    "        return text\n",
    "    \n",
    "    # run every-step\n",
    "    df[text_col] = [text_prepare(x) for x in df[text_col]]\n",
    "    \n",
    "    \n",
    "    # return processed df\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7bfbc62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Text Preprocessing\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>arcticfantasy would almost take offense actually snap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>illinoisloyalty rutgers game abomination affront god man must never speak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cozangaming lisa ask start rag call heh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sometimes get mad something minuscule try ruin somebodies life like lose job like get federal pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sometimes get mad something minuscule try ruin somebodies life like lose job like get federal pr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                  Text\n",
       "0                                                arcticfantasy would almost take offense actually snap\n",
       "1                            illinoisloyalty rutgers game abomination affront god man must never speak\n",
       "2                                                              cozangaming lisa ask start rag call heh\n",
       "3  sometimes get mad something minuscule try ruin somebodies life like lose job like get federal pr...\n",
       "4  sometimes get mad something minuscule try ruin somebodies life like lose job like get federal pr..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Text Preprocessing\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>arcticfantasy would almost take offense actually snap</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>illinoisloyalty rutgers game abomination affront god man must never speak</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cozangaming lisa ask start rag call heh</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sometimes get mad something minuscule try ruin somebodies life like lose job like get federal pr...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sometimes get mad something minuscule try ruin somebodies life like lose job like get federal pr...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                  Text  \\\n",
       "0                                                arcticfantasy would almost take offense actually snap   \n",
       "1                            illinoisloyalty rutgers game abomination affront god man must never speak   \n",
       "2                                                              cozangaming lisa ask start rag call heh   \n",
       "3  sometimes get mad something minuscule try ruin somebodies life like lose job like get federal pr...   \n",
       "4  sometimes get mad something minuscule try ruin somebodies life like lose job like get federal pr...   \n",
       "\n",
       "  Emotion  \n",
       "0   anger  \n",
       "1   anger  \n",
       "2   anger  \n",
       "3   anger  \n",
       "4   anger  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Before Text Preprocessing\")\n",
    "display(train_data.head()[['Text']])\n",
    "processed_df = text_preprocessing_platform(train_data, 'Text', remove_stopwords=True)\n",
    "print(\"After Text Preprocessing\")\n",
    "display(processed_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec4f147",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028c13f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8139d57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001661cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e29138",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91b3c7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d73727e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34af330e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7018f087",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afeaf3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5153ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c644b83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0df695",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ae5063",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing duplicated values\n",
    "index = train_data[train_data['Text'].duplicated() == True].index\n",
    "train_data.drop(index, axis = 0, inplace = True)\n",
    "train_data.reset_index(inplace=True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f38fc73c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                        would almost took offense actually snapped\n",
      "1                                 rutgers game abomination affront must never speak\n",
      "2                                              thats lisa asked started raging call\n",
      "3    sometimes something minuscule ruin somebody life like lose like federal prison\n",
      "4    sometimes something minuscule ruin somebody life like lose like federal prison\n",
      "5                     think must actually working like havent snap chat video today\n",
      "6                eye dilated hate world right rage thousand fiery dragon need drink\n",
      "7                               chosen member seat people dole mate elect candidate\n",
      "8                               chosen member seat people dole mate elect candidate\n",
      "9                                      please canadian player play player atrocious\n",
      "Name: CleanedText, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def clean_text_data(text):\n",
    "    # Apply preprocessor\n",
    "    text = p.clean(text)\n",
    "\n",
    "    # Remove HTML tags and URLs\n",
    "    text = re.sub(r'<[^>]+>|http[s]?://\\S+|http\\S+|www\\S+|https\\S+', '', text)\n",
    "    \n",
    "    # Remove punctuation and replace words with multiple consecutive letters\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'(\\w)\\1{2,}', r'\\1', text)\n",
    "    \n",
    "    # Insert a space before all capital letters in the middle of a sentence\n",
    "    text = re.sub(r\"(\\w)([A-Z])\", r\"\\1 \\2\", text)\n",
    "\n",
    "    # Tokenize the tweet using TweetTokenizer\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "    word_tokens = tokenizer.tokenize(text)\n",
    "\n",
    "    # Stop word removal and length filtering\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.remove('not')\n",
    "    filtered_text = [word for word in word_tokens if word.isalnum() and len(word) > 3 and word.lower() not in stop_words]\n",
    "\n",
    "    # Lowercase change\n",
    "    text = ' '.join(filtered_text).lower()\n",
    "\n",
    "    # Lemmatization using WordNet Lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in word_tokenize(text)]\n",
    "    text = ' '.join(lemmatized_words)\n",
    "\n",
    "    return text\n",
    "\n",
    "# Assuming 'df' is your DataFrame and 'TweetText' is the column to be cleaned\n",
    "train_data['CleanedText'] = train_data['Text'].apply(clean_text_data)\n",
    "\n",
    "# Display the cleaned DataFrame\n",
    "print(train_data['CleanedText'].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19868142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_df['Text'].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "181862ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>anger resentment hatred destroyer fortune today</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>1johndes ball watch rojo header equally dreadful</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>ccrago dreadful even meet catfish still think</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>hedgehogdylan would frown bite fold arm every time need assistance someone expect lil</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>dxfyinggrxvity frustration look elphaba frown aggravation high pitch voice grow</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>watch django unchain people may frown titter delight twenty-five</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                      Text  \\\n",
       "67                                         anger resentment hatred destroyer fortune today   \n",
       "272                                       1johndes ball watch rojo header equally dreadful   \n",
       "280                                          ccrago dreadful even meet catfish still think   \n",
       "288  hedgehogdylan would frown bite fold arm every time need assistance someone expect lil   \n",
       "295        dxfyinggrxvity frustration look elphaba frown aggravation high pitch voice grow   \n",
       "344                       watch django unchain people may frown titter delight twenty-five   \n",
       "\n",
       "     Emotion  \n",
       "67     anger  \n",
       "272  sadness  \n",
       "280  sadness  \n",
       "288  sadness  \n",
       "295  sadness  \n",
       "344  sadness  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_df[processed_df['Text'].duplicated() == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "540a4a48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>CleanedText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>@hesham786 that's the spirit #optimism</td>\n",
       "      <td>joy</td>\n",
       "      <td>thats spirit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>@hesham786 that's the spirit</td>\n",
       "      <td>joy</td>\n",
       "      <td>thats spirit</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Text Emotion   CleanedText\n",
       "241  @hesham786 that's the spirit #optimism     joy  thats spirit\n",
       "242           @hesham786 that's the spirit      joy  thats spirit"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print some of those rows to check\n",
    "train_data[train_data['CleanedText'] == train_data.iloc[242]['CleanedText']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ce578ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.drop_duplicates(subset='CleanedText', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c2d9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.pipeline import Pipeline# Apply text cleaning\n",
    "train_data['CleanedText'] = train_data['Text'].apply(clean_text_data)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_data['CleanedText'], train_data['Emotion'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply SMOTE to oversample the minority classes\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train.to_frame(), y_train)\n",
    "\n",
    "# Convert resampled X_train back to a Series\n",
    "X_train_resampled = X_train_resampled['CleanedText']\n",
    "\n",
    "# Define a pipeline with a TF-IDF vectorizer and a classifier (Naive Bayes, SVM, or Random Forest)\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('classifier', MultinomialNB())  # You can replace MultinomialNB with SVC or RandomForestClassifier\n",
    "])\n",
    "\n",
    "# Train the model on the resampled data\n",
    "pipeline.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Predictions\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05120701",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "train_data['Emotion_Label'] = label_encoder.fit_transform(train_data['Emotion'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "affb9695",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_data['CleanedText'], train_data['Emotion_Label'], test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d978ba20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ca5442a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.51\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.82      0.45      0.58        20\n",
      "        fear       0.44      0.68      0.53        25\n",
      "         joy       0.40      0.36      0.38        11\n",
      "     sadness       0.60      0.43      0.50        14\n",
      "\n",
      "    accuracy                           0.51        70\n",
      "   macro avg       0.56      0.48      0.50        70\n",
      "weighted avg       0.57      0.51      0.52        70\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Instantiate and train the Naive Bayes classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = nb_classifier.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "classification_rep = classification_report(y_test, y_pred, target_names=label_encoder.classes_)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d56fc98a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get         6.789505\n",
      "like        5.383700\n",
      "would       5.308323\n",
      "go          4.692508\n",
      "think       4.527916\n",
      "one         4.352122\n",
      "people      4.019874\n",
      "want        3.634914\n",
      "love        3.552992\n",
      "look        3.391168\n",
      "day         3.383057\n",
      "make        3.196956\n",
      "watch       3.195772\n",
      "awful       3.146595\n",
      "man         2.821896\n",
      "depress     2.624254\n",
      "optimism    2.538665\n",
      "feel        2.504959\n",
      "two         2.496961\n",
      "much        2.447746\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# TF IDF vectorizer with adjusted parameters\n",
    "tfidf_vect = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform\n",
    "matrix_tfidf = tfidf_vect.fit_transform(processed_df['Text'])\n",
    "\n",
    "# Using get_feature_names_out\n",
    "featureNames = tfidf_vect.get_feature_names_out()\n",
    "\n",
    "# Data frame for our matrix_tfidf and featureNames\n",
    "df_tfidf = pd.DataFrame(data=matrix_tfidf.toarray(), columns=featureNames)\n",
    "\n",
    "# Adding up the importance scores (= TF-IDF scores) for every word.\n",
    "wordScores = df_tfidf.sum(axis=0)\n",
    "\n",
    "# Sorting words according to how much they matter in all the tweets\n",
    "# Sorting them with their overall TF-IDF scores.\n",
    "top20words = wordScores.sort_values(ascending=False).head(20)\n",
    "\n",
    "# Print top20words\n",
    "print(top20words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c582b2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U imbalanced-learn scikit-learn\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# Oversample the training data\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X = train_data['CleanedText'].values.reshape(-1, 1)\n",
    "y = train_data['Emotion'].values\n",
    "X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "\n",
    "# convert text (object) data to string for w2v\n",
    "X_resampled= [str(obj) for obj in X_resampled]\n",
    "X_resampled = np.array(X_resampled)\n",
    "\n",
    "# resource : https://www.kaggle.com/code/titanpointe/cyberbullying-tweets-eda-automl-dl-bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "411c587b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Word2Vec Model\n",
    "sentences = [word_tokenize(text) for text in X_resampled]\n",
    "word2vec_model = Word2Vec(sentences, vector_size=300, window=5, min_count=1, workers=4)  # Adjust parameters as needed\n",
    "\n",
    "# Convert Text to Embeddings\n",
    "def get_embedding(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    # Filter out tokens that are not in the vocabulary\n",
    "    tokens = [token for token in tokens if token in word2vec_model.wv.key_to_index]\n",
    "    if len(tokens) > 0:\n",
    "        # Return the average of word embeddings for the tokens\n",
    "        return np.mean([word2vec_model.wv[t] for t in tokens], axis=0)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Create an array of embeddings for each text\n",
    "X_resampled = [get_embedding(text) for text in X_resampled]\n",
    "\n",
    "# resource: https://www.kaggle.com/code/titanpointe/cyberbullying-tweets-eda-automl-dl-bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4e73cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(processed_df['Text'], processed_df['Emotion'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Use TfidfVectorizer to convert text data to numerical features\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "324f0e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.85      0.55      0.67        20\n",
      "        fear       0.59      0.88      0.71        25\n",
      "         joy       0.57      0.73      0.64        11\n",
      "     sadness       0.83      0.36      0.50        14\n",
      "\n",
      "    accuracy                           0.66        70\n",
      "   macro avg       0.71      0.63      0.63        70\n",
      "weighted avg       0.71      0.66      0.64        70\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(processed_df['Text'], processed_df['Emotion'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Use TfidfVectorizer to convert text data to numerical features\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "\n",
    "# Create and train the Logistic Regression model\n",
    "logistic_regression_classifier = LogisticRegression(max_iter=1000)\n",
    "logistic_regression_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_logistic = logistic_regression_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Logistic Regression Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_logistic, zero_division=1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "47eb0f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.89      0.40      0.55        20\n",
      "        fear       0.50      1.00      0.67        25\n",
      "         joy       1.00      0.64      0.78        11\n",
      "     sadness       1.00      0.29      0.44        14\n",
      "\n",
      "    accuracy                           0.63        70\n",
      "   macro avg       0.85      0.58      0.61        70\n",
      "weighted avg       0.79      0.63      0.61        70\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create and train the Random Forest model\n",
    "random_forest_classifier = RandomForestClassifier(random_state=42)\n",
    "random_forest_classifier.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_rf = random_forest_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Random Forest Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf, zero_division=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c098ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.62      0.50      0.56        20\n",
      "        fear       0.56      0.96      0.71        25\n",
      "         joy       0.50      0.36      0.42        11\n",
      "     sadness       1.00      0.21      0.35        14\n",
      "\n",
      "    accuracy                           0.59        70\n",
      "   macro avg       0.67      0.51      0.51        70\n",
      "weighted avg       0.66      0.59      0.55        70\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create and train the Decision Tree model\n",
    "decision_tree_classifier = DecisionTreeClassifier(random_state=42)\n",
    "decision_tree_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_dt = decision_tree_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Decision Tree Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_dt, zero_division=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe3d1f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Machines Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       1.00      0.30      0.46        20\n",
      "        fear       0.49      0.92      0.64        25\n",
      "         joy       0.73      0.73      0.73        11\n",
      "     sadness       0.83      0.36      0.50        14\n",
      "\n",
      "    accuracy                           0.60        70\n",
      "   macro avg       0.76      0.58      0.58        70\n",
      "weighted avg       0.74      0.60      0.57        70\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create and train the Support Vector Machines model\n",
    "svm_classifier = SVC()\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_svm = svm_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Support Vector Machines Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_svm, zero_division=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154b3353",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f854ac0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d705a14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
